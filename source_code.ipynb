{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261daaeb-8b95-4264-b618-1ab0c3e0ef3f",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aba3461-beca-42ac-9371-2a7396a767b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch scikit-learn pandas streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857df0f-a5a1-400a-8412-f4014b504fb4",
   "metadata": {},
   "source": [
    "Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d522585-596d-4643-bcbe-2850bafa9ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91950\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch and Hugging Face\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# For Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05705fe3-9e18-45b7-be6b-856c7786b6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\F'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\F'\n",
      "C:\\Users\\91950\\AppData\\Local\\Temp\\ipykernel_19012\\286824153.py:2: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  true_df = pd.read_csv(\"Data\\True.csv\")\n",
      "C:\\Users\\91950\\AppData\\Local\\Temp\\ipykernel_19012\\286824153.py:3: SyntaxWarning: invalid escape sequence '\\F'\n",
      "  fake_df = pd.read_csv(\"Data\\Fake.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "true_df = pd.read_csv(\"Data\\True.csv\")\n",
    "fake_df = pd.read_csv(\"Data\\Fake.csv\")\n",
    "\n",
    "# Add labels: 1 for true, 0 for fake\n",
    "true_df[\"label\"] = 1\n",
    "fake_df[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2a7a0be-927f-4a09-8b45-9b437057315a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['politicsNews', 'worldnews'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df[\"subject\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d871d74-796b-4c5a-a601-54f874c5c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample randomly\n",
    "true_sample = true_df.sample(n=2000, random_state=42)\n",
    "fake_sample = fake_df.sample(n=2000, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "df = pd.concat([true_sample, fake_sample], ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6163d5-adec-4271-8a90-884cfc6a0711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May's government pushes Brexit bill to avoid '...</td>\n",
       "      <td>LONDON (Reuters) - Brexit minister David Davis...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 6, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump’s EPA OKs Pesticide That Causes Brain D...</td>\n",
       "      <td>Farmworkers were pulled from fields on Friday ...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 15, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man arrested at Trump rally said he wanted to ...</td>\n",
       "      <td>(Reuters) - A man arrested over the weekend tr...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 20, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jared Kushner NEVER Registered To Vote As A “F...</td>\n",
       "      <td>Meanwhile, as President Trump continues to mee...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Sep 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MARTHA STEWART Makes Lewd Gesture Towards Trum...</td>\n",
       "      <td>Martha, Martha, Martha You re 75-years old! Ti...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>May 8, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Arkansas attorney general says open to working...</td>\n",
       "      <td>(Reuters) - Arkansas Attorney General Leslie R...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 17, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>UK's May to meet Bill Clinton to discuss North...</td>\n",
       "      <td>LONDON (Reuters) - British Prime Minister Ther...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 18, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>Supreme Court dismisses Hawaii's challenge to ...</td>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. Supreme Court ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>October 24, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Watch Mitt Romney Totally Humiliate Himself I...</td>\n",
       "      <td>Mitt Romney got a lot of praise and a lot of h...</td>\n",
       "      <td>News</td>\n",
       "      <td>November 29, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>Watch: Trump Supporting Coal CEO Upset Trump ...</td>\n",
       "      <td>While on the campaign trail, Donald Trump prom...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 6, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     May's government pushes Brexit bill to avoid '...   \n",
       "1      Trump’s EPA OKs Pesticide That Causes Brain D...   \n",
       "2     Man arrested at Trump rally said he wanted to ...   \n",
       "3     Jared Kushner NEVER Registered To Vote As A “F...   \n",
       "4     MARTHA STEWART Makes Lewd Gesture Towards Trum...   \n",
       "...                                                 ...   \n",
       "3995  Arkansas attorney general says open to working...   \n",
       "3996  UK's May to meet Bill Clinton to discuss North...   \n",
       "3997  Supreme Court dismisses Hawaii's challenge to ...   \n",
       "3998   Watch Mitt Romney Totally Humiliate Himself I...   \n",
       "3999   Watch: Trump Supporting Coal CEO Upset Trump ...   \n",
       "\n",
       "                                                   text       subject  \\\n",
       "0     LONDON (Reuters) - Brexit minister David Davis...     worldnews   \n",
       "1     Farmworkers were pulled from fields on Friday ...          News   \n",
       "2     (Reuters) - A man arrested over the weekend tr...  politicsNews   \n",
       "3     Meanwhile, as President Trump continues to mee...     left-news   \n",
       "4     Martha, Martha, Martha You re 75-years old! Ti...     left-news   \n",
       "...                                                 ...           ...   \n",
       "3995  (Reuters) - Arkansas Attorney General Leslie R...  politicsNews   \n",
       "3996  LONDON (Reuters) - British Prime Minister Ther...     worldnews   \n",
       "3997  WASHINGTON (Reuters) - The U.S. Supreme Court ...  politicsNews   \n",
       "3998  Mitt Romney got a lot of praise and a lot of h...          News   \n",
       "3999  While on the campaign trail, Donald Trump prom...          News   \n",
       "\n",
       "                    date  label  \n",
       "0     September 6, 2017       1  \n",
       "1           May 15, 2017      0  \n",
       "2         June 20, 2016       1  \n",
       "3           Sep 29, 2017      0  \n",
       "4            May 8, 2017      0  \n",
       "...                  ...    ...  \n",
       "3995  November 17, 2016       1  \n",
       "3996   October 18, 2017       1  \n",
       "3997   October 24, 2017       1  \n",
       "3998   November 29, 2016      0  \n",
       "3999    December 6, 2017      0  \n",
       "\n",
       "[4000 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save as CSV\n",
    "df.to_csv(\"small_news_dataset.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a747fe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91950\\miniconda3\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\91950\\miniconda3\\lib\\site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\91950\\miniconda3\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91950\\miniconda3\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91950\\miniconda3\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91950\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91950\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91950\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91950\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (2025.6.15)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\91950\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\91950\\miniconda3\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91950\\miniconda3\\Lib\\site-packages\\gdown\\parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/u/2/folders/1odGBNOdeNMbidYc_aLSyjO6ZweeNzkaL\n",
      "To: c:\\Users\\91950\\Desktop\\Build a Transformer-Based Fake News Detection Chatbot\\small_news_dataset.csv\n",
      "1.12MB [00:00, 4.27MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'small_news_dataset.csv'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/drive/u/2/folders/1odGBNOdeNMbidYc_aLSyjO6ZweeNzkaL\"\n",
    "output = \"small_news_dataset.csv\"\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f235494-2bc2-476c-bed5-736963b0c38c",
   "metadata": {},
   "source": [
    "#### Use text as the primary input.\n",
    "#### Optionally concatenate title + text to give model both headline and body.\n",
    "#### Avoid subject and date for now (they introduce bias and require extra encoding logic).\n",
    "#### Why not subject/date?- Transformers handle context in language, not structured metadata like dates. Including subject might make the model lazy (e.g., if all fake news is under one subject, it just memorizes that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04cca3a9-9b1d-4ea5-8ccb-aa1f9658db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and text into one column\n",
    "df[\"content\"] = df[\"title\"].astype(str) + \" \" + df[\"text\"].astype(str)\n",
    "\n",
    "# Keep only combined text and label\n",
    "df = df[[\"content\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1e9ff5-eedc-46a4-a37c-4612caf7d961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May's government pushes Brexit bill to avoid '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump’s EPA OKs Pesticide That Causes Brain D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man arrested at Trump rally said he wanted to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jared Kushner NEVER Registered To Vote As A “F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MARTHA STEWART Makes Lewd Gesture Towards Trum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Arkansas attorney general says open to working...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>UK's May to meet Bill Clinton to discuss North...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>Supreme Court dismisses Hawaii's challenge to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Watch Mitt Romney Totally Humiliate Himself I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>Watch: Trump Supporting Coal CEO Upset Trump ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  label\n",
       "0     May's government pushes Brexit bill to avoid '...      1\n",
       "1      Trump’s EPA OKs Pesticide That Causes Brain D...      0\n",
       "2     Man arrested at Trump rally said he wanted to ...      1\n",
       "3     Jared Kushner NEVER Registered To Vote As A “F...      0\n",
       "4     MARTHA STEWART Makes Lewd Gesture Towards Trum...      0\n",
       "...                                                 ...    ...\n",
       "3995  Arkansas attorney general says open to working...      1\n",
       "3996  UK's May to meet Bill Clinton to discuss North...      1\n",
       "3997  Supreme Court dismisses Hawaii's challenge to ...      1\n",
       "3998   Watch Mitt Romney Totally Humiliate Himself I...      0\n",
       "3999   Watch: Trump Supporting Coal CEO Upset Trump ...      0\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6c68f-aed7-4173-8d40-d27c179161b9",
   "metadata": {},
   "source": [
    "###  Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71993fde-ccf1-4e9d-af66-6c203b331ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove non-alphanumeric characters (keep spaces)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to your combined text\n",
    "df.loc[:, \"content\"] = df[\"content\"].apply(lambda x: clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea0a289e-fba9-41b5-92a5-c74ce72115fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mays government pushes Brexit bill to avoid ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trumps EPA OKs Pesticide That Causes Brain Dam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man arrested at Trump rally said he wanted to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jared Kushner NEVER Registered To Vote As A Fe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MARTHA STEWART Makes Lewd Gesture Towards Trum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  Mays government pushes Brexit bill to avoid ch...      1\n",
       "1  Trumps EPA OKs Pesticide That Causes Brain Dam...      0\n",
       "2  Man arrested at Trump rally said he wanted to ...      1\n",
       "3  Jared Kushner NEVER Registered To Vote As A Fe...      0\n",
       "4  MARTHA STEWART Makes Lewd Gesture Towards Trum...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bb5bd-863f-4c57-a8ed-837e0808a717",
   "metadata": {},
   "source": [
    "###  Split into Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c85df64-16fe-45c6-bff9-1121f0cee9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3200\n",
      "Val size: 400\n",
      "Test size: 400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split into train and temp (val+test)\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df[\"content\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# Split temp into validation and test\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_texts))\n",
    "print(\"Val size:\", len(val_texts))\n",
    "print(\"Test size:\", len(test_texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6bb34-3df5-49a0-936d-f2ce7e93957a",
   "metadata": {},
   "source": [
    "### Model Selection and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e15074-af87-4e1f-9b92-fbd19d4e865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f75783e-6dec-48b1-966d-cb0af2fac51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f51de1-9e9c-471d-af63-d13d10649e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,  # shorter for faster training\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_texts)\n",
    "val_encodings = tokenize_data(val_texts)\n",
    "test_encodings = tokenize_data(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e935e5a3-f2b8-4c66-b21d-0e5b079aa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "val_dataset = NewsDataset(val_encodings, val_labels)\n",
    "test_dataset = NewsDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c63d5-a5bc-43e0-ada8-a36b3ed70956",
   "metadata": {},
   "source": [
    "###  Fine-tune DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f1b45-b73f-4de3-9652-18d8744cecbd",
   "metadata": {},
   "source": [
    "#### 1. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e3e9a7b-1526-468a-9ee2-ad980cce2f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2  # binary classification (fake or true)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5305e1bf-52c4-4ef0-8439-8f0cbc9c8ac1",
   "metadata": {},
   "source": [
    "#### 2. Define Metrics for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67bebf75-8755-4db1-81c2-864c5db0ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0224909-088e-42c1-b4da-21c4e0af0092",
   "metadata": {},
   "source": [
    "#### 3. Set Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb9e4d48-1d03-4f4c-b326-cbbf2d117e55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m training_args = TrainingArguments(\n\u001b[32m      4\u001b[39m     output_dir=\u001b[33m'\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m'\u001b[39m,              \u001b[38;5;66;03m# Where model checkpoints will go\u001b[39;00m\n\u001b[32m      5\u001b[39m     num_train_epochs=\u001b[32m2\u001b[39m,                  \u001b[38;5;66;03m# Feel free to increase if time allows\u001b[39;00m\n\u001b[32m      6\u001b[39m     per_device_train_batch_size=\u001b[32m8\u001b[39m,       \n\u001b[32m      7\u001b[39m     per_device_eval_batch_size=\u001b[32m8\u001b[39m,\n\u001b[32m      8\u001b[39m     warmup_steps=\u001b[32m50\u001b[39m,\n\u001b[32m      9\u001b[39m     weight_decay=\u001b[32m0.01\u001b[39m,\n\u001b[32m     10\u001b[39m     logging_dir=\u001b[33m'\u001b[39m\u001b[33m./logs\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     logging_steps=\u001b[32m10\u001b[39m,\n\u001b[32m     12\u001b[39m     do_train=\u001b[38;5;28;01mTrue\u001b[39;00m,                       \u001b[38;5;66;03m# Explicitly tell it to train\u001b[39;00m\n\u001b[32m     13\u001b[39m     do_eval=\u001b[38;5;28;01mTrue\u001b[39;00m                         \u001b[38;5;66;03m# Explicitly tell it to evaluate\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:133\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91950\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1793\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1791\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m   1795\u001b[39m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.average_tokens_across_devices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91950\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:2322\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2318\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2319\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2320\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2321\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2322\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._setup_devices\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91950\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:76\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     74\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     cached = \u001b[38;5;28mself\u001b[39m.fget(obj)\n\u001b[32m     77\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91950\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:2192\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2191\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2192\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2193\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2194\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2195\u001b[39m         )\n\u001b[32m   2196\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2197\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',              # Where model checkpoints will go\n",
    "    num_train_epochs=2,                  # Feel free to increase if time allows\n",
    "    per_device_train_batch_size=8,       \n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    do_train=True,                       # Explicitly tell it to train\n",
    "    do_eval=True                         # Explicitly tell it to evaluate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cfe44-bf0d-4727-ac50-75178d6f98fa",
   "metadata": {},
   "source": [
    "####  4. Train the Model with HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9655c71-ca51-4718-8442-4c223bcb2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03419b06-7477-44fd-a451-276c7ae85c0a",
   "metadata": {},
   "source": [
    "#### 5. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b171d8-eadc-4395-a01f-36b8572c1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91950\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 2:15:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=0.047589722672710194, metrics={'train_runtime': 8137.6264, 'train_samples_per_second': 0.786, 'train_steps_per_second': 0.098, 'total_flos': 423895675699200.0, 'train_loss': 0.047589722672710194, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbeebf-ef96-4ff5-86ba-4bae619a0412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a9df1-9d36-4455-b7b6-afea1da03ea7",
   "metadata": {},
   "source": [
    "#### 6. Evaluate on Test Set (after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ac4af-3e3c-401a-9d95-afffdbeb9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91950\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.035448167473077774,\n",
       " 'eval_accuracy': 0.995,\n",
       " 'eval_f1': 0.995,\n",
       " 'eval_precision': 0.995,\n",
       " 'eval_recall': 0.995,\n",
       " 'eval_runtime': 99.8491,\n",
       " 'eval_samples_per_second': 4.006,\n",
       " 'eval_steps_per_second': 0.501,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61324d-7fd7-410f-8be0-8034f22d4c27",
   "metadata": {},
   "source": [
    "#### 7: Save the Trained Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9c395-2cd7-4823-a789-326f8f896592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved_model\\\\tokenizer_config.json',\n",
       " 'saved_model\\\\special_tokens_map.json',\n",
       " 'saved_model\\\\vocab.txt',\n",
       " 'saved_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a folder to save model and tokenizer\n",
    "model_path = \"saved_model\"\n",
    "\n",
    "# Save both model and tokenizer\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098bb4a7-5e27-4bf9-952a-61f7dd1322cc",
   "metadata": {},
   "source": [
    "###  Chatbot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c93b9-ed7b-4819-99a4-44b0ea341f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load model + tokenizer\n",
    "def load_model(model_path=\"saved_model\"):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return tokenizer, model\n",
    "\n",
    "# Predict function\n",
    "def classify_news(text, tokenizer, model):\n",
    "    # Preprocess the input text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        confidence, predicted_class = torch.max(probs, dim=1)\n",
    "\n",
    "    label_map = {0: \"Fake\", 1: \"True\"}\n",
    "    predicted_label = label_map[predicted_class.item()]\n",
    "    confidence_percent = confidence.item() * 100\n",
    "\n",
    "    # Conversational response\n",
    "    response = f\"This news is likely {predicted_label.upper()} with a confidence of {confidence_percent:.2f}%.\"\n",
    "\n",
    "    return response, predicted_label, confidence_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f59d5f-d6da-46d3-9490-7c5dc45fc0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste your news snippet:  trump imposed 60% tarrifs on india\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This news is likely TRUE with a confidence of 99.55%.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tokenizer, model = load_model()\n",
    "    user_input = input(\"Paste your news snippet: \")\n",
    "    response, label, conf = classify_news(user_input, tokenizer, model)\n",
    "    print(\"\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8b9df-efbe-496d-95aa-f1e953cb6baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addac61e-1fc4-44a5-a6bd-a48aac2df1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b738b-4ce7-484b-a310-cb22ecc3793b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c51738-0d12-4631-a4cc-deda1d26ab7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
